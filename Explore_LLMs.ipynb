{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5533f5a2",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c80b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Tokenization ---\n",
    "# In a real model, this is a complex tokenizer. We'll fake it.\n",
    "sentence = \"the cat sat on the mat\"\n",
    "tokens = sentence.split(' ')\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5000ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'on': 0, 'sat': 1, 'cat': 2, 'the': 3, 'mat': 4}\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary: a map from token to a unique ID\n",
    "vocab = {token: i for i, token in enumerate(set(tokens))}\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc99f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 0, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32ed605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3, 2, 1, 0, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to their numerical IDs\n",
    "token_ids = [vocab[token] for token in tokens]\n",
    "print(f\"Token IDs: {token_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3b5759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Table (a learned lookup table):\n",
      "[[0.37454012 0.95071431 0.73199394 0.59865848]\n",
      " [0.15601864 0.15599452 0.05808361 0.86617615]\n",
      " [0.60111501 0.70807258 0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911 0.18182497 0.18340451]\n",
      " [0.30424224 0.52475643 0.43194502 0.29122914]]\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Embedding ---\n",
    "# Let's define the size of our embedding vectors (the \"meaning space\" dimension)\n",
    "embedding_dim = 4\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# An embedding table is just a lookup matrix.\n",
    "# Each row corresponds to a token ID's vector.\n",
    "# In a real model, these numbers are learned during training.\n",
    "np.random.seed(42)\n",
    "embedding_table = np.random.rand(vocab_size, embedding_dim)\n",
    "print(\"\\nEmbedding Table (a learned lookup table):\")\n",
    "print(embedding_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2954a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence as a sequence of embeddings (shape: (6, 4)):\n",
      "[[0.83244264 0.21233911 0.18182497 0.18340451]\n",
      " [0.60111501 0.70807258 0.02058449 0.96990985]\n",
      " [0.15601864 0.15599452 0.05808361 0.86617615]\n",
      " [0.37454012 0.95071431 0.73199394 0.59865848]\n",
      " [0.83244264 0.21233911 0.18182497 0.18340451]\n",
      " [0.30424224 0.52475643 0.43194502 0.29122914]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Convert our token IDs into their embedding vectors\n",
    "sentence_embeddings = embedding_table[token_ids]\n",
    "print(f\"\\nSentence as a sequence of embeddings (shape: {sentence_embeddings.shape}):\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfede3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings with Absolute Positional info:\n",
      "(6, 4)\n"
     ]
    }
   ],
   "source": [
    "# Absolute positional embedings\n",
    "import numpy as np\n",
    "\n",
    "# Let's use the embeddings from Chapter 1\n",
    "# sentence: \"the cat sat on the mat\"\n",
    "# sentence_embeddings shape: (6, 4) -> 6 tokens, 4 dimensions each\n",
    "sentence_embeddings = np.random.rand(6, 4)\n",
    "seq_len, embedding_dim = sentence_embeddings.shape\n",
    "\n",
    "# --- 1. Absolute Positional Embeddings ---\n",
    "# Create a learnable table for positions\n",
    "max_seq_len = 10 # Model can handle up to 10 tokens\n",
    "positional_embedding_table = np.random.rand(max_seq_len, embedding_dim)\n",
    "\n",
    "# Get the positional vectors for our sequence (positions 0 through 5)\n",
    "positional_encodings = positional_embedding_table[:seq_len, :]\n",
    "\n",
    "# Add them to the word embeddings\n",
    "final_embeddings_abs = sentence_embeddings + positional_encodings\n",
    "print(\"Final embeddings with Absolute Positional info:\")\n",
    "print(final_embeddings_abs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae92147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25877998, 0.66252228, 0.31171108, 0.52006802],\n",
       "       [0.54671028, 0.18485446, 0.96958463, 0.77513282],\n",
       "       [0.93949894, 0.89482735, 0.59789998, 0.92187424],\n",
       "       [0.0884925 , 0.19598286, 0.04522729, 0.32533033],\n",
       "       [0.38867729, 0.27134903, 0.82873751, 0.35675333],\n",
       "       [0.28093451, 0.54269608, 0.14092422, 0.80219698],\n",
       "       [0.07455064, 0.98688694, 0.77224477, 0.19871568],\n",
       "       [0.00552212, 0.81546143, 0.70685734, 0.72900717],\n",
       "       [0.77127035, 0.07404465, 0.35846573, 0.11586906],\n",
       "       [0.86310343, 0.62329813, 0.33089802, 0.06355835]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "positional_embedding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa238dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25877998, 0.66252228, 0.31171108, 0.52006802],\n",
       "       [0.54671028, 0.18485446, 0.96958463, 0.77513282],\n",
       "       [0.93949894, 0.89482735, 0.59789998, 0.92187424],\n",
       "       [0.0884925 , 0.19598286, 0.04522729, 0.32533033],\n",
       "       [0.38867729, 0.27134903, 0.82873751, 0.35675333],\n",
       "       [0.28093451, 0.54269608, 0.14092422, 0.80219698]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encodings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
